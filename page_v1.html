<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
  /*@import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);*/
  @import url(https://fonts.googleapis.com/css?family=Lato:400,400italic,500,500italic,700,700italic);
  /*@import url(https://fonts.googleapis.com/css?family=Lato:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);*/
  /* @import url(https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons); */
    a {
    color: #0051c9;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    /*font-family: 'Roboto', sans-serif;*/
    /*font-family: 'Lato', sans-serif;*/
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    font-size: 14px;
    font-weight: 400;
    }
    strong {
    /*font-family: 'Roboto', sans-serif;*/
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    /*font-family: 'Lato', sans-serif;*/
    font-size: 14.5px;
    font-weight: 500;
    }
    heading {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    /*font-family: 'Roboto', sans-serif;*/
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 24px;
    font-weight: 700;
    }
    papertitle {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'Lato', sans-serif;*/
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    /*font-family: 'Roboto', sans-serif;*/
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 15px;
    font-weight:600;
    }
    author {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    /*font-family: 'Roboto', sans-serif;*/
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 13.0px;
    font-weight:500;
    }
    name {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'BlinkMacSystemFont', sans-serif;*/
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    /*font-family: 'Roboto', sans-serif;*/
    /*font-family: 'Avenir Next';*/
    font-weight: 500;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 140px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="media/preview.jpg">
  <title>Kim Youwang</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="script/functions.js"></script>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Kim Youwang</name>
            <br>
            <span style="color:#7F7F7F;"><em>youwang.kim@postech.ac.kr</em></span>
        </p>
        <p>
          I am a Ph.D. student at <a href="https://ami.postech.ac.kr/">AMILab</a> in Electrical Engineering at <a href="https://postech.ac.kr/eng">POSTECH</a>, supervised by <a href="https://ami.postech.ac.kr/members/tae-hyun-oh">Tae-Hyun Oh</a>.
            I am also an incoming research scientist intern at <a href="https://about.meta.com/realitylabs/"> Meta Reality Labs</a>, hosted by <a href="https://www.linkedin.com/in/yaser-sheikh-9847a64/">Yaser Sheikh</a>.
            I was a visiting researcher at <a href="https://virtualhumans.mpi-inf.mpg.de/">Real Virtual Humans Group</a> at the <a href="https://uni-tuebingen.de/">Univ. of Tübingen</a>, working with <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a>.

        </p>
        <p>
            I received my bachelor's degree in Electrical Engineering from POSTECH. During my bachelor, I spent a semester at <a href="https://www.ntu.edu.sg/">Nanyang Technological University (NTU)</a> as an exchange student.
        </p>

        <p align=center>
<!--          <a href="mailto:youwang.kim@postech.ac.kr">Email</a> &nbsp|&nbsp-->
          <a href="https://www.overleaf.com/read/prgrxsxzmkwd">CV</a> &nbsp|&nbsp
          <a href="https://github.com/Youwang-Kim">GitHub</a> &nbsp|&nbsp
          <a href="https://scholar.google.com/citations?user=gKXTrF8AAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
          <a href="https://www.linkedin.com/in/kim-youwang/"> LinkedIn</a>
          &nbsp|&nbsp
          <a href="https://twitter.com/kim_youwang">Twitter</a>
        </p>
        </td>
        <td width="33%">
          <img src="media/profile_231009.jpeg" width="224" alt="headshot">
        </td>
        <!--<td width="33%">
        <img src="media/bw_headshot.png" width="250">
        <img src="media/color_headshot.png" width="250">
        </td>-->
      </tr>
      </table>

<!--      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td width="10%" valign="middle">-->
<!--          <a href="https://postech.ac.kr"><img src="media/postech_logo.png" width="120"></a>-->
<!--        </td>-->
<!--        <td width="10%" valign="middle">-->
<!--          <a href="https://ami.postech.ac.kr/"><img src="media/ami_logo.png" width="60"></a>-->
<!--        </td>-->
<!--&lt;!&ndash;        <td width="10%" valign="middle">&ndash;&gt;-->
<!--&lt;!&ndash;          <a href="https://research.google/"><img src="media/google_logo.png" width="55"></a>&ndash;&gt;-->
<!--&lt;!&ndash;        </td>&ndash;&gt;-->
<!--&lt;!&ndash;        <td width="10%" valign="middle">&ndash;&gt;-->
<!--&lt;!&ndash;          <a href="https://about.facebook.com/realitylabs/"><img src="media/frl_logo.png" width="60"></a>&ndash;&gt;-->
<!--&lt;!&ndash;        </td>&ndash;&gt;-->
<!--&lt;!&ndash;        <td width="10%" valign="middle">&ndash;&gt;-->
<!--&lt;!&ndash;          <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="90"></a>&ndash;&gt;-->
<!--&lt;!&ndash;        </td>&ndash;&gt;-->
<!--&lt;!&ndash;        <td width="10%" valign="middle">&ndash;&gt;-->
<!--&lt;!&ndash;          <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="60"></a>&ndash;&gt;-->
<!--&lt;!&ndash;        </td>	&ndash;&gt;-->
<!--&lt;!&ndash;        <td width="10%" valign="middle">&ndash;&gt;-->
<!--&lt;!&ndash;          <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="35"></a></td>&ndash;&gt;-->
<!--&lt;!&ndash;        </td>&ndash;&gt;-->
<!--&lt;!&ndash;        <td width="10%" valign="middle">&ndash;&gt;-->
<!--&lt;!&ndash;          <a href="https://en.xjtu.edu.cn/"><img src="media/xjtu_logo.png" width="50"></a></td>&ndash;&gt;-->
<!--&lt;!&ndash;        </td>	&ndash;&gt;-->
<!--        -->
<!--        &lt;!&ndash; <td width="10%" valign="middle">-->
<!--          <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>-->
<!--        </td>-->
<!--        <td width="10%" valign="middle">-->
<!--          <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="80"></a>-->
<!--        </td>-->
<!--        <td width="10%" valign="middle">-->
<!--          <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="100"></a>-->
<!--        </td>-->
<!--        <td width="10%" valign="middle">-->
<!--          <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="70"></a>-->
<!--        </td>	-->
<!--        <td width="10%" valign="middle">-->
<!--          <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="40"></a></td>-->
<!--        </td>-->
<!--        <td width="10%" valign="middle">-->
<!--          <a href="https://en.xjtu.edu.cn/"><img src="media/xjtu_logo.png" width="60"></a></td>-->
<!--        </td>	 &ndash;&gt;-->
<!--      </tr>-->
<!--      </table>-->




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <heading>News</heading>
            <ul>
                <li><strong>02/2024 <span style="color:#ff0000;">(New!)</span></strong> Our paper on <a href="https://arxiv.org/abs/2312.11360">text-driven PBR texture synthesis</a> is accepted to CVPR 2024.</li>
                <li><strong>02/2024 <span style="color:#ff0000;">(New!)</span> </strong> I'll be joining <a href="https://about.meta.com/realitylabs/">Meta Reality Labs - Codec Avatars team</a> as a research scientist intern, this fall. </li>
<!--                <li><strong>12/2023</strong> Our paper on <a href="https://arxiv.org/abs/2312.11360">text-driven PBR texture synthesis</a> is now live on arXiv. </li>-->
                <li><strong>12/2023</strong> Our paper on efficient 3D scene stylization is accepted to AAAI 2024. </li>
                <li><strong>11/2023</strong> I won the Grand Prize (1st place, Minister's award, $12,000) at ICT paper awards. </li>
                <li><strong>10/2023</strong> I am selected as an <a href="https://ami.postech.ac.kr/57da3912-6dc3-41a1-b355-f010231a1756">outstanding reviewer</a> for ICCV 2023. </li>
                <li><strong>10/2023</strong> I am joining <a href="https://virtualhumans.mpi-inf.mpg.de/">Real Virtual Humans Group</a> as a visiting Ph.D. student. </li>
<!--                TODO: show more toggle button-->
              <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
              <div id="old_news" style="display: none;">
                <li><strong>08/2023</strong> Our paper on text-driven human avatar generation is accepted to ICCVW-AI3DCC 2023. </li>
                <li><strong>04/2023</strong> Our paper on 3D face mesh recon. on videos is accepted to CVPRW-3DMV 2023. </li>
                <li><strong>02/2023</strong> Invited to give a talk on text-driven 4D human avatars at <a href="http://innerverz.io/">INNERVERZ</a>. </li>
                <li><strong>01/2023</strong> Our <a href="https://link.springer.com/article/10.1007/s00371-023-02798-x">paper</a> on lightweight body mesh recon. is accepted to TVCJ 2023. </li>
                <li><strong>11/2022</strong> Our paper <a href="https://clip-actor.github.io">CLIP-Actor</a> is selected as the winner of <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-south-korea">Qualcomm Innovation Fellowship Korea</a>!</li>
                <li><strong>07/2022</strong> Our two papers, <a href="https://clip-actor.github.io">CLIP-Actor</a> and <a href="https://fastmetro.github.io">FastMETRO</a>, are accepted to ECCV 2022.</li>
                <li><strong>04/2022</strong> I got accepted to <a href="https://iplab.dmi.unict.it/icvss2022/">ICVSS 2022</a>, and I will visit Sicily, Italy this summer. </li>
                <li><strong>10/2021</strong> Our paper <a href="https://demr.github.io">DeMR</a> is accepted to BMVC 2021.</li>
              </div>
            </ul>
        </td>
      </tr>
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
            <p>I am interested in <strong>3D/4D virtual humans, animals, and objects</strong>. My goal is to enrich a virtual world through realistic and creative <strong>3D/4D content generation</strong>.
                Currently, I'm focusing on the topics below, but not limited to.
            </p>
            <ul>
            <li>Realistic 3D/4D human, animal and object generation for virtual world.</li>
            <li>Accurate 3D pose and shape reconstruction of humans and animals.</li>
            </ul>
        </td>
      </tr>

<!--      <tr>-->
<!--        <td width="100%" valign="middle">-->
<!--          <heading>Publications</heading>-->
<!--            <br>-->
<!--            (Equal contributions are denoted by *)-->
<!--        </td>-->
<!--      </tr>-->
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tr>
          <td width="30%">
              <img src='./media/paint_it.png' width="192" height="108">
          </td>
<!--            <video width="192" height="108" muted autoplay loop poster="" controlsList="nodownload" >-->
<!--            <source src="./media/paint-it.mp4" type="video/mp4" />-->
<!--            </video>-->
          <td valign="top" width="75%">
<!--              <a href="">-->
                <papertitle>
                  &#127912 Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering
                </papertitle>
<!--          <br><br>-->
          <p style="margin-top:0.15cm;margin-bottom:0.2cm">
              <u>Kim Youwang</u>,
              Tae-Hyun Oh,
              Gerard Pons-Moll
          <br>
              <span style="color:#7F7F7F;"><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024</span>
<!--              CVPR 2024-->
          <br>
            <a href="https://kim-youwang.github.io/paint-it">[Project page]</a>
            <a href="https://arxiv.org/abs/2312.11360">[Paper]</a>
            <a href="https://github.com/postech-ami/paint-it">[Code]</a>
          </p>
            • Synthesizing realistic 3D mesh PBR texture from a text.
          </td>
        </tr>

               <tr>
          <td width="30%">
              <img src='./media/clip-actor-x.jpg' width="192" height="146">
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  CLIP-Actor-X: Text-driven 4D Human Avatar Generation via Cross-modal Synthesis-through-Optimization
                </papertitle>
          <br>
          <p style="margin-top:0.15cm;margin-bottom:0.2cm">
              <u>Kim Youwang</u>*,
              Taehyun Byun*,
              Kim Ji-Yeon,
              Sungjoon Choi,
              Tae-Hyun Oh
          <br>
              <span style="color:#7F7F7F;"><em>Journal under review</em></span>
          <br>
          </p>
            • An improved text-to-4D textured human avatar synthesis.
          </td>
         </tr>

       <tr>
          <td width="30%">
            <img src='media/syn3d.png' width="192" height="128">
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  ObjectDR: Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild
                </papertitle>
              <br>
              <p style="margin-top:0.15cm;margin-bottom:0.2cm">
                  Junhyeong Cho,
                  <u>Kim Youwang</u>,
                  Hunmin Yang,
                  Tae-Hyun Oh
              <br>
                  <span style="color:#7F7F7F;"><em>arXiv preprint, 2024</em></span>
              <br>
                <a href="https://objectdr.github.io/">[Project page]</a> <a href="https://arxiv.org/abs/2403.14539">[Paper]</a>
              </p>
                    • Learns to capture a domain-invariant geometry prior via synthetic 3D dataset.
<!--                • Reliable 3D face mesh labels for large-scale facial video datasets.-->
<!--          <p>-->
<!--          </p>-->
          </td>
        </tr>


        <tr>
          <td width="30%">
            <img src='./media/obj_mesh.gif' width="192" height="133">
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  (Ongoing) Monocular 3D object mesh reconstruction
                </papertitle>
          <p>
            • Reconstructing 3D textured mesh from a single image.
          </p>
          </td>
        </tr>


        <tr>
          <td width="30%">
            <img src='./media/fprf.gif' width="192" height="144">
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  Feed-Forward Photorealistic Style Transfer for Large-Scale 3D Neural Radiance Fields
                </papertitle>
                <br>
          <p style="margin-top:0.15cm;margin-bottom:0.2cm">
              GeonU Kim,
              <u>Kim Youwang</u>,
              Tae-Hyun Oh
               <br>
              <span style="color:#7F7F7F;"><em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2024</span>
<!--              AAAI 2024-->
          <br>
              <a href="https://kim-geonu.github.io/FPRF/">[Project page]</a>
              <a href="https://arxiv.org/abs/2401.05516">[Paper]</a>
              <a href="https://github.com/postech-ami/FPRF">[Code]</a>
<!--          </p>-->
          </p>
            • Efficient feed-forward stylization method for 3D scenes.
          </td>
        </tr>

<!--      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >-->
        <tr>
          <td width="30%">
            <img src='./media/4d_face.gif' width="192" height="128">
          </td>
          <td valign="top" width="75%">
<!--              <a href="">-->
                <papertitle>
                  A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization
                </papertitle>
<!--              </a>-->
                        <br>
          <p style="margin-top:0.15cm;margin-bottom:0.2cm">
              <u>Kim Youwang</u>,
              Lee Hyun*,
              Kim Sung-Bin*,
              Suekyeong Nam,
              Janghoon Ju,
              Tae-Hyun Oh
          <br>
              <span style="color:#7F7F7F;"><em>arXiv preprint, 2023</em></span>
          <br>
            <a href="https://neuface-dataset.github.io">[Project page]</a> <a href="https://arxiv.org/abs/2310.03205">[Paper]</a>
          </p>
            • Reliable 3D face mesh labels for large-scale facial video datasets.
          </td>
        </tr>

<!--        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >-->
        <tr>
          <td width="30%">
            <img src='./media/tex_avatar.gif' width="192" height="108">
          </td>
          <td valign="top" width="75%">
    <!--              <a href="">-->
                <papertitle>
                  Text-driven Human Avatar Generation by Neural Re-parameterized Texture Optimization
                </papertitle>
    <!--              </a>-->
          <p style="margin-top:0.15cm;margin-bottom:0.2cm">
              <u>Kim Youwang</u>,
              Tae-Hyun Oh
          <br>
              <span style="color:#7F7F7F;"><em>IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</em>, 2023</span>
              <br>
            </p>
            • Generating high quality 4D human avatars from a single text.
          </p>
          </td>
        </tr>


        <tr>
          <td width="30%">
            <img src='./media/stream.gif' width="192" height="128">
          </td>
          <td valign="top" width="75%">
<!--              <a href="">-->
                <papertitle>
                    STREAM: Spatio-Temporally Consistent Face Mesh Reconstruction on Videos
                </papertitle>
<!--              </a>-->
          <p style="margin-top:0.15cm;margin-bottom:0.2cm">
              <u>Kim Youwang</u>,
              Lee Hyun*,
              Kim Sung-Bin*,
              Suekyeong Nam,
              Janghoon Ju,
              Tae-Hyun Oh
          <br>
              <span style="color:#7F7F7F;"><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, 2023</span>
<!--              3DMV: Learning 3D with Multi-View Supervision (in conj. with CVPR)</em>, 2023-->
            <br>
          </p>
            • Reconstructing multi-view & temporally consistent 3D face meshes on videos.
          </td>
        </tr>

        <tr>
          <td width="30%">
            <img src='media/rankpruning.jpg' width="175" height="134">
          </td>
          <td valign="top" width="75%">
                <papertitle>Multi-stage Adaptive Rank Statistic Pruning for Lightweight Human 3D Mesh Recovery Model</papertitle>
          <p style="margin-top:0.15cm;margin-bottom:0.2cm">
              Dong Hun Ryou, <u>Kim Youwang</u>, Tae-Hyun Oh
          <br>
              <span style="color:#7F7F7F;"><em>The Visual Computer Journal (TVCJ), 2023</em></span>
          <br>
            <a href="https://link.springer.com/article/10.1007/s00371-023-02798-x">[Paper]</a>
          </p>
            • Neural network pruning for efficient 3D human mesh reconstruction.
          </td>
        </tr>



          <tr>
            <td width="30%">
              <img src='./media/clip_actor.gif' width="192" height="108">
            </td>
            <td valign="top" width="75%">
                  <papertitle>
                    CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes
                  </papertitle>
            <p style="margin-top:0.15cm;margin-bottom:0.2cm">
                <u>Kim Youwang</u>*,
                Kim Ji-Yeon*,
                Tae-Hyun Oh
            <br>
                <span style="color:#7F7F7F;"><em>European Conference on Computer Vision (ECCV)</em>, 2022</span>
<!--            <br>-->
            <br>
              <a href="https://clip-actor.github.io">[Project page]</a>
              <a href="https://arxiv.org/abs/2206.04382">[Paper]</a>
              <a href="https://youtu.be/oWr4NP-eVLY">[Video]</a>
              <a href="https://github.com/postech-ami/CLIP-Actor">[Code]</a>
              <a href="https://www.dropbox.com/s/8l2jvvc0po6szn7/3229-poster.pdf?dl=0">[Poster]</a>
            </p>
              • Text-to-4D textured human avatar synthesis.
            <br>
                <span style="color:#ff0000;">- Grand Prize (1st place, Minister's award, $12,000) at ICT paper awards 2023 </span>
            <br>
                <span style="color:#ff0000;">- Qualcomm Innovation Award Winner 2022</span>
            </td>
          </tr>


          <tr>
            <td width="30%">
              <img src='media/fastmetro_attention.png' width="192" height="125">
            </td>
            <td valign="top" width="75%">
                  <papertitle>
                    FastMETRO: Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers
                  </papertitle>
            <p style="margin-top:0.15cm;margin-bottom:0.2cm">
                Junhyeong Cho,
                <u>Kim Youwang</u>,
                Tae-Hyun Oh
            <br>
                <span style="color:#7F7F7F;"><em>European Conference on Computer Vision (ECCV)</em>, 2022</span>
            <br>
              <a href="https://fastmetro.github.io/">[Project page]</a>
              <a href="https://arxiv.org/abs/2207.13820">[Paper]</a>
              <!-- <a href="https://youtu.be/V5hYTz5os0M">video</a> | -->
              <a href="https://github.com/postech-ami/FastMETRO">[Code]</a>
              <a href="https://www.dropbox.com/s/kzmihz488qcelxi/2116-poster.pdf?dl=0">[Poster]</a>
              </p>
              • A lightweight and fast transformer for human mesh reconstruction.
            </td>
          </tr>

          <tr>
            <td width="30%">
              <img src='media/demr.jpg' width="192" height="100">
            </td>
            <td valign="top" width="75%">
                  <papertitle>
                    Unified 3D Mesh Recovery of Humans and Animals by Learning Animal Exercise
                  </papertitle>
            <br>
            <p style="margin-top:0.15cm;margin-bottom:0.2cm">
                <u>Kim Youwang</u>,
                Kim Ji-Yeon,
                Kyungdon Joo,
                Tae-Hyun Oh
            <br>
                <span style="color:#7F7F7F;"><em>British Machine Vision Conference (BMVC)</em>, 2021</span>
            <br>
              <a href="https://demr.github.io/">[Project page]</a>
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0926.pdf">[Paper]</a>
            </p>
                • An efficient multi-task model for multi-class mesh reconstruction.
            </td>
          </tr>
        </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
        <tr>
        <td>
          <heading>Invited Talks</heading>
            <ul>
              <li>Towards Efficient & Realistic Virtual World Communication, <em>INNERVERZ, Korea</em>, 2023</li>
<!--            </div></div>-->
            </ul>
        </td>
      </tr>

      <tr>
        <td>
          <heading>Academic Services</heading>
            <ul>
                <li> Conference Reviewer: ICCV (<span style="color:#ff0000;">Outstanding reviewer</span> in 2023), CVPR (2024), ECCV (2024)
                    <br>
                <li> Journal Reviewer: TMM (2023)
<!--            </div></div>-->
            </ul>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
      <tr>
        <td>
          <heading>Misc.</heading>
            <ul>
                Come visit my <a href="https://www.youtube.com/@kimyouwang"> Youtube channel</a>! I regularly upload paper reviews (in Korean).
            </ul>
        </td>
        <td>
            <br>
            <ul>
                +) As a guitarist of the <a href="https://www.youtube.com/@postechsteeler1150">band STEELER</a>, I also upload some music covers or gigs.
            </ul>
        </td>
      </tr>
      <tr>
            <td width="25%">
                <iframe width="384" height="216" src="https://www.youtube.com/embed/FSG5bCkNWWo" title="NeRF seminar" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>
            <td width="25%">
                <iframe width="384" height="216" src="https://www.youtube.com/embed/cg7x99Vs7f0" title="Steven Wilson - 3 Years Older (Cover by Steeler)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>
        </tr>
      </table>

      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          template adapted from <a href="https://jonbarron.info/"><font size="2">this awesome website</font></a>
          <!-- <br> -->
          <!-- Last updated: Mar 2023 -->
        </font>
        </p>
        </td>
      </tr>
  </table>

<!--  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->

<!--        <tr>-->
<!--            <td>-->
<!--                <heading>Misc.</heading>-->
<!--                <br>-->
<!--                Come visit my <a href="https://www.youtube.com/@kimyouwang"> Youtube channel</a>! I regularly upload paper reviews (in Korean).-->

<!--            </td>-->
<!--            <td>-->
<!--                <br>-->
<!--                +) As a guitarist of the <a href="https://www.youtube.com/@postechsteeler1150">band STEELER</a>, I also upload some music covers or gigs.-->
<!--            </td>-->
<!--        </tr>-->
<!--        <tr>-->
<!--            <td width="25%">-->
<!--                <iframe width="384" height="216" src="https://www.youtube.com/embed/FSG5bCkNWWo" title="NeRF seminar" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>-->
<!--            </td>-->
<!--            <td width="25%">-->
<!--                <iframe width="384" height="216" src="https://www.youtube.com/embed/cg7x99Vs7f0" title="Steven Wilson - 3 Years Older (Cover by Steeler)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>-->
<!--            </td>-->
<!--        </tr>-->
<!--&lt;!&ndash;    </table>&ndash;&gt;-->



<!--      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td>-->
<!--        <br>-->
<!--        <p align="right">-->
<!--          <font size="2">-->
<!--          template adapted from <a href="https://jonbarron.info/"><font size="2">this awesome website</font></a>-->
<!--          &lt;!&ndash; <br> &ndash;&gt;-->
<!--          &lt;!&ndash; Last updated: Mar 2023 &ndash;&gt;-->
<!--        </font>-->
<!--        </p>-->
<!--        </td>-->
<!--      </tr>-->
<!--      </table>-->
  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Invited Talks</heading>
        </td>
      </tr>
  </table> -->

  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"> -->
<!--    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;">-->
<!--    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--        <tr>-->
<!--            <heading>Invited Talks</heading>-->
<!--                <ul>-->
<!--                    <li><strong>Towards Efficient & Realistic Virtual World Communication</strong>, <em>INNERVERZ, Korea</em>, 2023</li>-->
<!--                </ul>-->
<!--            <br><br>-->
<!--        </tr>-->
<!--    </table>-->
<!--      &lt;!&ndash; Teaching &ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--          <heading>Teaching</heading>-->
<!--          <tr>-->
<!--            <td style="padding:0px;width:25%;vertical-align:middle">-->
<!--              <img src="media/eth_logo.png" width="180">-->
<!--            </td>-->
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              Teaching Assistant (Lead), <a href="https://www.cvg.ethz.ch/teaching/3dvision/"><strong>3D Vision</strong></a>, Spring 2023-->
<!--                <br>-->
<!--              Teaching Assistant, <a href="https://cvg.ethz.ch/teaching/compvis/"><strong>Computer Vision</strong></a>, Fall 2022-->
<!--              <br>-->
<!--              Teaching Assistant (Lead), <a href="https://www.cvg.ethz.ch/teaching/3dvision/2022/index.php"><strong>3D Vision</strong></a>, Spring 2022-->
<!--              <br>-->
<!--              Teaching Assistant, <a href="https://www.cvg.ethz.ch/teaching/dlseminar/"><strong>Deep Learning for Computer Vision: Seminal Work</strong></a>, Spring 2022-->
<!--              <br>-->
<!--              Teaching Assistant, <a href="https://www.cvg.ethz.ch/teaching/3dvision/2020/index.php"><strong>3D Vision</strong></a>, Spring 2020-->
<!--              <br>-->
<!--              Teaching Assistant, <a href="https://www.cvg.ethz.ch/teaching/dlseminar/2020/"><strong>Deep Learning for Computer Vision: Seminal Work</strong></a>, Spring 2020-->
<!--              <br>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </table>-->
<!--        <br>-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--          <tr>-->
<!--            <td style="padding:0px;width:25%;vertical-align:middle">-->
<!--              <img src="media/tue_logo.png" width="180">-->
<!--            </td>-->
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              Teaching Assistant, <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/deep-learning/"><strong>Deep Learning</strong></a>, Winter 2020/2021-->
<!--              <br>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </table>-->
<!--      <br>-->
<!--      <br>-->
<!--      <br>-->

<!--      &lt;!&ndash; Academic Services &ndash;&gt;-->
<!--      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--        <heading>Academic Services</heading>-->
<!--          <tr>-->
<!--            <td>-->
<!--              <ul>-->
<!--                  <li> <strong>Conference Reviewer</strong>: ICCV (2023)<br>-->
<!--                  <li> <strong>Journal Reviewer</strong>: TMM (2023)-->
<!--              </ul>-->
<!--            </td>-->
<!--          </tr>-->
<!--      </table>-->

<!--    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      </table>-->
        <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5RW688RKHW"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-5RW688RKHW');
    </script>
<!--      <script type="text/javascript">-->
<!--      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");-->
<!--          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));-->

<!--      </script> <script type="text/javascript">-->
<!--      try {-->
<!--          var pageTracker = _gat._getTracker("UA-116734954-1");-->
<!--          pageTracker._trackPageview();-->
<!--          } catch(err) {}-->
<!--      </script>-->
      <!-- Global site tag (gtag.js) - Google Analytics -->
<!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>-->
<!--<script>-->
<!--  window.dataLayer = window.dataLayer || [];-->
<!--  function gtag(){dataLayer.push(arguments);}-->
<!--  gtag('js', new Date());-->

<!--  gtag('config', 'UA-116734954-1');-->
<!--</script>-->
<!--    </td>-->
<!--    </tr>-->
<!--  </table>-->
  </body>
</html>
<!--  -->
