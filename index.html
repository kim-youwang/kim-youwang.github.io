<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <title>Kim Youwang</title>
    <link rel="stylesheet" type="text/css" id="defaultstyle" href="./style.css"/>
    <script type="text/javascript" src="js/tracker.js"></script>
    <script type="text/javascript" src="js/pageturner.js"></script>
  </head>
  <script src="script/functions.js"></script>
  <body>
    <div class="container">
      <div class="intro">
        <div class="top-section">
          <img src="media/profile_250517.jpg" alt="Kim Youwang">
          <div class="text-info">
            <h1>Kim Youwang</h1>
            <small style="color:gray">youwang.kim@postech.ac.kr</small>
            Ph.D. Student, Electrical Engineering, POSTECH
            <div class="top-links">
              <a href="media/cv/CV_KimYouwang_250425.pdf" target="_blank">CV</a> |
              <a href="media/youwang_goal.mp4" target="_blank">Research Summary</a> |
              <a href="https://scholar.google.com/citations?user=gKXTrF8AAAAJ&hl=en" target="_blank">Google Scholar</a> |
              <a href="https://www.linkedin.com/in/kim-youwang/" target="_blank">LinkedIn</a> |
              <a href="https://github.com/Youwang-Kim" target="_blank">GitHub</a>
            </div>
          </div>
        </div>

        <div class="bio-text">
          <p>
            I am a Ph.D. student at <a href="https://ami.kaist.ac.kr/" target="_blank">AMILab</a> at <a href="https://kaist.ac.kr/en" target="_blank">KAIST</a>, advised by <a href="https://ami.kaist.ac.kr/members/tae-hyun-oh" target="_blank">Tae-Hyun Oh</a>.
            I was a research scientist intern at <a href="https://about.meta.com/realitylabs/codecavatars/?utm_source=about.facebook.com&utm_medium=redirect" target="_blank"> Meta Codec Avatars team</a>, working with <a href="https://sites.google.com/site/zjucaochen/home" target="_blank">Chen Cao</a>, <a href="https://www.linkedin.com/in/jovan-cmu/" target="_blank">Jovan Popović</a>, and <a href="https://www.linkedin.com/in/yaser-sheikh-9847a64/" target="_blank">Yaser Sheikh</a>.
            I also work closely with <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank">Gerard Pons-Moll</a>, and was a visiting researcher at <a href="https://virtualhumans.mpi-inf.mpg.de/" target="_blank">Real Virtual Humans Group</a> at the <a href="https://uni-tuebingen.de/" target="_blank">Univ. of Tübingen</a>.
            I received my bachelor's degree in Electrical Engineering from POSTECH.
          </p>
          <p>
            I work on research projects at the intersection of computer graphics, vision, and machine learning.
            My research goal is to achieve a foundational understanding of the visual world through realistic generation and reconstruction of 3D agents (e.g., humans, animals, objects, and robots) from sparse observations, enabling embodied intelligence.
          </p>
        </div>
      </div>

        <h2>Research Experiences</h2>
        <ul>
            <li class="exp_list"><b>Meta Reality Labs - Codec Avatars team</b>, Pittsburgh, PA, USA.<span style="float: right;">Oct 2024 - Mar 2025</span><br>
                &nbsp&nbsp Research scientist intern, working with <a href="https://sites.google.com/site/zjucaochen/home" target="_blank">Chen Cao</a>, <a href="https://www.linkedin.com/in/jovan-cmu/" target="_blank">Jovan Popović</a>, and <a href="https://www.linkedin.com/in/yaser-sheikh-9847a64/" target="_blank">Yaser Sheikh</a>.<br>
            </li>
            <li class="exp_list"><b>University of Tübingen - Real Virtual Humans group</b>, Tübingen, Germany.<span style="float: right;">Sep 2023 - Present</span><br>
                &nbsp&nbsp Worked as a visiting researcher. Ongoing research collaboration with <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank">Gerard Pons-Moll</a>.<br/>
            </li>
            <li class="exp_list"><b>POSTECH - Advanced Machine Intelligence lab</b>, Pohang, Korea.<span style="float: right;">Sep 2020 - Present</span><br>
                &nbsp&nbsp Graduate student, working with <a href="https://ami.kaist.ac.kr/members/tae-hyun-oh" target="_blank">Tae-Hyun Oh</a>.<br/>
            </li>
        </ul>

      <h2>Publications</h2>

      <div class="item">
        <img src='./media/face_avatar.gif'/>
        <p>
          <b class="title">A Paper on Monocular 3D Face Avatar Reconstruction</b><br/>
          <br>
          <i>To be updated</i>
        </p>
      </div>

      <div class="item">
        <img src='./media/fpgs.gif'/>
        <p>
          <b class="title">FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of Large-Scale Gaussian Splatting</b><br/>
          GeonU Kim, <u>Kim Youwang</u>, Lee Hyoseok, Tae-Hyun Oh<br/>
          <i>arXiv 2025</i><br/>
            <a href="https://kim-geonu.github.io/FPGS/" target="_blank">Project page</a> |
            <a href="https://arxiv.org/abs/2503.09635" target="_blank">Paper</a> |
            <a href="https://github.com/kaist-ami/" target="_blank">Code(TBA)</a><br>
          <small style="color:gray;margin-bottom:0.0em">- Excellence Prize ($2K) at ICT paper awards 2024 </small><br/>
        </p>
      </div>

      <div class="item">
        <img src='./media/robust3drecon.png'/>
        <p>
          <b class="title">Robust 3D Shape Reconstruction in Zero-Shot from a Single Image in the Wild</b><br/>
          Junhyeong Cho, <u>Kim Youwang</u>, Hunmin Yang, Tae-Hyun Oh<br/>
          <i>CVPR 2025</i><br/>
            <a href="https://zeroshape-w.github.io/" target="_blank">Project page</a> |
            <a href="https://arxiv.org/abs/2403.14539v2" target="_blank">Paper</a><br>
        </p>
      </div>

      <div class="item">
        <img src='./media/4d_face.gif'/>
        <p>
          <b class="title">A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization</b><br/>
          <u>Kim Youwang</u>, Lee Hyun*, Kim Sung-Bin*, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh<br/>
          <i>ICLR 2025 / TMLR 2024</i><br/>
            <a href="https://kim-youwang.github.io/neuface" target="_blank">Project page</a> |
            <a href="https://openreview.net/forum?id=zVDMh6JvWc" target="_blank">Paper</a> |
            <a href="https://github.com/kaist-ami/NeuFace" target="_blank">Code</a><br>
          <small style="color:gray;margin-bottom: 0.0em">- Invited to ICLR 2025 for poster presentation (Top 5% TMLR papers invited)</small><br>
        </p>
      </div>

      <div class="item">
        <img src='./media/paint_it.jpg'/>
        <p>
          <b class="title">Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering</b><br/>
          <u>Kim Youwang</u>, Tae-Hyun Oh, Gerard Pons-Moll<br/>
          <i>CVPR 2024</i><br/>
            <a href="https://kim-youwang.github.io/paint-it" target="_blank">Project page</a> |
            <a href="https://arxiv.org/abs/2312.11360" target="_blank">Paper</a> |
            <a href="https://youtu.be/uSKK-ekVJLg" target="_blank">Video</a> |
            <a href="https://github.com/kaist-ami/paint-it" target="_blank">Code</a> |
            <a href="https://www.dropbox.com/scl/fi/4plpanxqy0lo16d3d8t8p/cvpr24_poster_youwang_final.pdf?rlkey=gah19krwju0clqdsep6g3qd4w&st=x831q8a5&dl=0" target="_blank">Poster</a><br/>
          <small style="color:gray;margin-bottom: 0.0em">- Best Poster Award at POSTECH-KAIST joint ML workshop 2024</small><br>
          <small style="color:gray;margin-bottom: 0.0em">- Presented in AI for Content Creation Workshop (AI4CC) at CVPR 2024</small><br>
          <small style="color:gray;margin-bottom: 0.0em">- Presented in AI for 3D Generation Workshop (AI3DG) at CVPR 2024</small>
        </p>
      </div>

      <div class="item">
        <img src='./media/metta.gif'/>
        <p>
          <b class="title">MeTTA: Single-View to 3D Textured Mesh Reconstruction with Test-Time Adaptation</b><br/>
          Kim Yu-Ji, Hyunwoo Ha, <u>Kim Youwang</u>, Jaeheung Surh, Hyowon Ha, Tae-Hyun Oh<br>
          <i>BMVC 2024 (<span style="color:#c00000;">Best Poster Award</span>)</i><br/>
            <a href="https://metta3d.github.io/" target="_blank">Project page</a><br>
          <small style="color:gray;margin-bottom: 0.0em">- Best Poster Award at BMVC 2024</small><br>
        </p>
      </div>

<!--      <div class="item">-->
<!--        <img src='./media/syn3d.png'/>-->
<!--        <p>-->
<!--          <b class="title">ObjectDR: Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild</b><br/>-->
<!--          Junhyeong Cho, <u>Kim Youwang</u>, Hunmin Yang, Tae-Hyun Oh<br/>-->
<!--          <i>CVPRW 2024</i><br/>-->
<!--            <a href="https://objectdr.github.io/" target="_blank">Project page</a> |-->
<!--            <a href="https://arxiv.org/abs/2403.14539v1" target="_blank">Paper</a>-->
<!--        </p>-->
<!--      </div>-->

      <div class="item">
        <img src='./media/clip-actor-x_crop.jpg'/>
        <p>
          <b class="title">CLIP-Actor-X: Text-driven 4D Human Avatar Generation via Cross-modal Synthesis-through-Optimization</b><br/>
          <u>Kim Youwang</u>*, Taehyun Byun*, Kim Ji-Yeon, Sungjoon Choi, Tae-Hyun Oh<br/>
          <i>Journal under review</i><br/>
        </p>
      </div>

      <div class="item">
        <img src='./media/fprf_v2.gif'/>
        <p>
          <b class="title">Feed-Forward Photorealistic Style Transfer for Large-Scale 3D Neural Radiance Fields</b><br/>
          GeonU Kim, <u>Kim Youwang</u>, Tae-Hyun Oh<br/>
          <i>AAAI 2024</i><br/>
            <a href="https://kim-geonu.github.io/FPRF/" target="_blank">Project page</a> |
            <a href="https://arxiv.org/abs/2401.05516" target="_blank">Paper</a> |
            <a href="https://github.com/kaist-ami/FPRF" target="_blank">Code</a><br>
        </p>
      </div>

<!--      <div class="item">-->
<!--        <img src='./media/tex_avatar.gif'/>-->
<!--        <p>-->
<!--          <b class="title">Text-driven Human Avatar Generation by Neural Re-parameterized Texture Optimization</b><br/>-->
<!--          <u>Kim Youwang</u>, Tae-Hyun Oh<br/>-->
<!--          <i>ICCVW 2023</i><br/>-->
<!--            <a href="https://ai3dcc.github.io/papers/0042_camready.pdf" target="_blank">Paper</a> |-->
<!--            <a href="https://ai3dcc.github.io/posters/0042_poster.pdf" target="_blank">Poster</a><br/>-->

<!--        </p>-->
<!--      </div>-->

<!--      <div class="item">-->
<!--        <img src='./media/stream.gif'/>-->
<!--        <p>-->
<!--          <b class="title">STREAM: Spatio-Temporally Consistent Face Mesh Reconstruction on Videos</b><br/>-->
<!--          <u>Kim Youwang</u>, Lee Hyun*, Kim Sung-Bin*, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh<br/>-->
<!--          <i>CVPRW 2023</i><br/>-->
<!--          <a href="https://3dmv2023.github.io/assets/posters/14.pdf" target="_blank">Poster</a><br/>-->
<!--        </p>-->
<!--      </div>-->

      <div class="item">
        <img src='./media/rank_pruning.jpg'/>
        <p>
          <b class="title">Multi-stage Adaptive Rank Statistic Pruning for Lightweight Human 3D Mesh Recovery Model</b><br/>
          Dong Hun Ryou, <u>Kim Youwang</u>, Tae-Hyun Oh<br/>
          <i>The Visual Computer Journal (TVCJ) 2023</i><br/>
            <a href="https://link.springer.com/article/10.1007/s00371-023-02798-x" target="_blank">Paper</a>
        </p>
      </div>

      <div class="item">
        <img src='./media/clip_actor.gif'/>
        <p>
          <b class="title">CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes</b><br/>
          <u>Kim Youwang</u>*, Kim Ji-Yeon*, Tae-Hyun Oh<br/>
          <i>ECCV 2022</i><br/>
            <a href="https://clip-actor.github.io" target="_blank">Project page</a> |
            <a href="https://arxiv.org/abs/2206.04382" target="_blank">Paper</a> |
            <a href="https://youtu.be/oWr4NP-eVLY" target="_blank">Video</a> |
            <a href="https://github.com/kaist-ami/CLIP-Actor" target="_blank">Code</a> |
            <a href="https://www.dropbox.com/s/8l2jvvc0po6szn7/3229-poster.pdf?dl=0" target="_blank">Poster</a><br/>
            <small style="color:gray;margin-bottom:0.0em">- Grand Prize (1st place, Minister's award, $12K) at ICT paper awards 2023 </small><br/>
            <small style="color:gray;margin-bottom:0.0em">- Qualcomm Innovation Award Winner 2022</small><br/>
            <small style="color:gray;margin-bottom:0.0em">- Presented in AI for Content Creation Workshop (AI4CC) at CVPR 2023</small>
        </p>
      </div>

      <div class="item">
        <img src='./media/fastmetro_teaser.jpg'/>
        <p>
          <b class="title">FastMETRO: Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers</b><br/>
          Junhyeong Cho, <u>Kim Youwang</u>, Tae-Hyun Oh<br/>
          <i>ECCV 2022</i><br/>
            <a href="https://fastmetro.github.io/" target="_blank">Project page</a> |
            <a href="https://arxiv.org/abs/2207.13820" target="_blank">Paper</a> |
            <a href="https://github.com/kaist-ami/FastMETRO" target="_blank">Code</a> |
            <a href="https://www.dropbox.com/s/kzmihz488qcelxi/2116-poster.pdf?dl=0" target="_blank">Poster</a><br/>
        </p>
      </div>

      <div class="item">
        <img src='./media/demr.jpg'/>
        <p>
        <b class="title">Unified 3D Mesh Recovery of Humans and Animals by Learning Animal Exercise</b><br/>
        <u>Kim Youwang</u>, Kim Ji-Yeon, Kyungdon Joo, Tae-Hyun Oh<br/>
            <i>BMVC 2021</i><br/>
            <a href="https://demr-bmvc21.github.io" target="_blank">Project page</a> |
            <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0926.pdf" target="_blank">Paper</a>
        </p>
      </div>

      <h2>Awards & Honors</h2>
        <ul>
            <li class="yw_list">Best Poster Award, BMVC, 2024</li>
            <li class="yw_list">Excellence Prize, Electronics Times ICT Paper Awards, 2024</li>
            <li class="yw_list">Best Poster Award, POSTECH-KAIST joint ML workshop, 2024</li>
            <li class="yw_list">Grand Prize (Minister's award, $12,000 prize), Electronics Times ICT Paper Awards, 2023</li>
            <li class="yw_list">Outstanding Reviewer Award, ICCV, 2023</li>
            <li class="yw_list">Winner ($4,000 prize), Qualcomm Innovation Fellowship Korea (QIFK), 2022</li>
            <li class="yw_list">International Computer Vision Summer School (ICVSS), 2022</li>
        </ul>

<!--      <h2>Talks</h2>-->
<!--        <ul>-->
<!--            <li class="yw_list">Towards Efficient & Realistic Virtual World Communication, INNERVERZ, Korea, 2023</li>-->
<!--        </ul>-->

      <h2>Academic Services</h2>
        <ul>
            <li class="yw_list">
              <b>Conference Reviewer</b>
              <ul>
                <li class="yw_list">CVPR: 2024, 2025</li>
                <li class="yw_list">ICCV: 2023 (<a href="https://iccv2023.thecvf.com/outstanding.reviewers-118.php">Outstanding reviewer</a>), 2025 </li>
                <li class="yw_list">ECCV: 2024</li>
                <li class="yw_list">NeurIPS: 2024, 2025</li>
                <li class="yw_list">BMVC: 2024</li>
                <br>
              </ul>
            </li>
            <li class="yw_list">
              <b>Journal Reviewer</b>
              <ul>
                <li class="yw_list">IEEE Trans. on Pattern Analysis and Machine Learning (TPAMI): 2024, 2025</li>
                <li class="yw_list">ACM Trans. on Graphics / SIGGRAPH Asia: 2024</li>
                <li class="yw_list">International Journal of Computer Vision (IJCV): 2024, 2025</li>
                <li class="yw_list">Transactions on Machine Learning Research (TMLR): 2025</li>
                <li class="yw_list">IEEE Transactions on Multimedia (TMM): 2023</li>
              </ul>
            </li>
        </ul>

    </div> <!-- container -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5RW688RKHW"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-5RW688RKHW');
    </script>
  </body>
</html>
