<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <title>Kim Youwang</title>
    <link rel="stylesheet" type="text/css" id="defaultstyle" href="./style.css"/>
    <script type="text/javascript" src="js/tracker.js"></script>
    <script type="text/javascript" src="js/pageturner.js"></script>
  </head>
  <script src="script/functions.js"></script>
  <body>
    <div class="container">
      <div class="intro">
        <div class="top-section">
          <img src="media/profile_250910.jpg" alt="Kim Youwang">
          <div class="text-info">
            <h1>Kim Youwang</h1>
            <small style="color:gray">youwang.kim@postech.ac.kr</small>
            Ph.D. Student, Electrical Engineering, POSTECH
            <div class="top-links">
              <a href="media/cv/CV_KimYouwang_251013.pdf" target="_blank">CV</a> |
              <a href="media/youwang_goal.mp4" target="_blank">Research Summary</a> |
              <a href="https://scholar.google.com/citations?user=gKXTrF8AAAAJ&hl=en" target="_blank">Google Scholar</a> |
              <a href="https://www.linkedin.com/in/kim-youwang/" target="_blank">LinkedIn</a> |
              <a href="https://github.com/Youwang-Kim" target="_blank">GitHub</a>
            </div>
          </div>
        </div>

        <div class="bio-text">
          <p>
            I am a Ph.D. student at <a href="https://www.postech.ac.kr/eng/index.do#;" target="_blank">POSTECH</a>, advised by <a href="https://ami.kaist.ac.kr/members/tae-hyun-oh" target="_blank">Tae-Hyun Oh</a>.
            My research aims to advance photorealistic virtual simulation and experiences by developing neural methods for synthesizing realistic 3D/4D digital twins from limited data and observations.
          </p>
          <p>
            During my PhD period, I was fortunate to work as a research scientist intern at the <a href="https://about.meta.com/realitylabs/codecavatars/?utm_source=about.facebook.com&utm_medium=redirect" target="_blank">Meta Codec Avatars team</a>,
            working with <a href="https://sites.google.com/site/zjucaochen/home" target="_blank">Chen Cao</a>, <a href="https://www.linkedin.com/in/jovan-cmu/" target="_blank">Jovan Popović</a>, and <a href="https://www.linkedin.com/in/yaser-sheikh-9847a64/" target="_blank">Yaser Sheikh</a>.
            I also work closely with <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank">Gerard Pons-Moll</a> and was a visiting researcher at the <a href="https://virtualhumans.mpi-inf.mpg.de/" target="_blank">Real Virtual Humans Group</a> at the <a href="https://uni-tuebingen.de/" target="_blank">Univ. of Tübingen</a>.
            I received my bachelor's degree in Electrical Engineering from POSTECH.
          </p>
        </div>
      </div>

      <h2>Experiences</h2>
      <ul>
          <li class="exp_list"><b>Meta Reality Labs - Codec Avatars team</b>, Pittsburgh, PA, USA.<span style="float: right;">Oct 2024 - Mar 2025</span><br>
              &nbsp&nbsp Research scientist intern, working with <a href="https://sites.google.com/site/zjucaochen/home" target="_blank">Chen Cao</a>, <a href="https://www.linkedin.com/in/jovan-cmu/" target="_blank">Jovan Popović</a>, and <a href="https://www.linkedin.com/in/yaser-sheikh-9847a64/" target="_blank">Yaser Sheikh</a>.<br>
          </li>
          <li class="exp_list"><b>University of Tübingen - Real Virtual Humans group</b>, Tübingen, Germany.<span style="float: right;">Sep 2023 - Present</span><br>
              &nbsp&nbsp Worked as a visiting researcher. Ongoing research collaboration with <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank">Gerard Pons-Moll</a>.<br/>
          </li>
          <li class="exp_list"><b>POSTECH - Advanced Machine Intelligence lab</b>, Pohang, Korea.<span style="float: right;">Sep 2020 - Present</span><br>
              &nbsp&nbsp Graduate student, working with <a href="https://ami.kaist.ac.kr/members/tae-hyun-oh" target="_blank">Tae-Hyun Oh</a>.<br/>
          </li>
      </ul>


      <div class="publications-header">
        <h2>Research</h2>
        <div class="pub-buttons">
          <button id="selected-btn" class="pub-button active-button">Selected</button>
          <button id="all-btn" class="pub-button">All</button>
        </div>
      </div>
      I'm interested in computer vision, graphics and generative modeling. My research is mainly about synthesizing realistic 3D/4D digital twins for humans, animals, objects, robots and scenes, from limited data and observations.

      <div class="publications-section">
        <div class="item selected">
          <img src='./media/youwang_avatar.gif'/>
          <p>
            <b class="title">A Paper on Efficient Personalized 3D Face Avatar Synthesis</b><br/>
            <br>
            <i>To be updated</i>
          </p>
        </div>


        <div class="item selected">
          <img src='./media/face_avatar.gif'/>
          <p>
            <b class="title">A Paper on Feed-forward 3D Face Avatar Synthesis</b><br/>
            <br>
            <i>To be updated</i>
          </p>
        </div>

        <div class="item selected">
          <img src='./media/gen_human.png'/>
          <p>
            <b class="title">Dress-up: Generating Animatable Clothed 3D Humans via Latent Modeling of 3D Gaussian Texture Maps</b><br/>
            <u>Kim Youwang</u>, Lee Hyoseok, Gerard Pons-Moll, Tae-Hyun Oh<br/>
            <br>
            <i>ICCVw 2025 (<span style="color:#c00000;">Oral presentation</span>)</i><br/>
            <small style="color:gray;margin-bottom:0.0em">- Workshop on Computer Vision for Fashion, Art, and Design</small><br/>
          </p>
        </div>

        <div class="item">
          <img src='./media/clip-actor-x_crop.jpg'/>
          <p>
            <b class="title">CLIP-Actor-X: Text-driven 4D Human Avatar Generation via Cross-modal Synthesis-through-Optimization</b><br/>
            <u>Kim Youwang</u>*, Taehyun Byun*, Kim Ji-Yeon, Sungjoon Choi, Tae-Hyun Oh<br/>
            <i>TPAMI - under major revision</i><br/>
          </p>
        </div>

        <div class="item selected">
          <img src='./media/fpgs.gif'/>
          <p>
            <b class="title">FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of Large-Scale Gaussian Splatting</b><br/>
            GeonU Kim, <u>Kim Youwang</u>, Lee Hyoseok, Tae-Hyun Oh<br/>
            <i>IJCV - under major revision</i><br/>
              <a href="https://kim-geonu.github.io/FPGS/" target="_blank">Project page</a> |
              <a href="https://arxiv.org/abs/2503.09635" target="_blank">Paper</a> |
              <a href="https://github.com/kaist-ami/" target="_blank">Code(TBA)</a><br>
            <small style="color:gray;margin-bottom:0.0em">- Excellence Prize ($2K) at ICT paper awards 2024 </small><br/>
          </p>
        </div>

        <div class="item">
          <img src='./media/robust3drecon.png'/>
          <p>
            <b class="title">Robust 3D Shape Reconstruction in Zero-Shot from a Single Image in the Wild</b><br/>
            Junhyeong Cho, <u>Kim Youwang</u>, Hunmin Yang, Tae-Hyun Oh<br/>
            <i>CVPR 2025</i><br/>
              <a href="https://zeroshape-w.github.io/" target="_blank">Project page</a> |
              <a href="https://arxiv.org/abs/2403.14539v2" target="_blank">Paper</a><br>
          </p>
        </div>

        <div class="item selected">
          <img src='./media/4d_face.gif'/>
          <p>
            <b class="title">A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization</b><br/>
            <u>Kim Youwang</u>, Lee Hyun*, Kim Sung-Bin*, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh<br/>
            <i>ICLR 2025 / TMLR 2024</i><br/>
              <a href="https://kim-youwang.github.io/neuface" target="_blank">Project page</a> |
              <a href="https://openreview.net/forum?id=zVDMh6JvWc" target="_blank">Paper</a> |
              <a href="https://github.com/kaist-ami/NeuFace" target="_blank">Code</a><br>
            <small style="color:gray;margin-bottom: 0.0em">- Invited to ICLR 2025 for poster presentation (Top 5% TMLR papers invited)</small><br>
          </p>
        </div>

        <div class="item selected">
          <img src='./media/paint_it.jpg'/>
          <p>
            <b class="title">Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering</b><br/>
            <u>Kim Youwang</u>, Tae-Hyun Oh, Gerard Pons-Moll<br/>
            <i>CVPR 2024</i><br/>
              <a href="https://kim-youwang.github.io/paint-it" target="_blank">Project page</a> |
              <a href="https://arxiv.org/abs/2312.11360" target="_blank">Paper</a> |
              <a href="https://youtu.be/uSKK-ekVJLg" target="_blank">Video</a> |
              <a href="https://github.com/kaist-ami/paint-it" target="_blank">Code</a> |
              <a href="https://www.dropbox.com/scl/fi/4plpanxqy0lo16d3d8t8p/cvpr24_poster_youwang_final.pdf?rlkey=gah19krwju0clqdsep6g3qd4w&st=x831q8a5&dl=0" target="_blank">Poster</a><br/>
            <small style="color:gray;margin-bottom: 0.0em">- Best Poster Award at POSTECH-KAIST joint ML workshop 2024</small><br>
            <small style="color:gray;margin-bottom: 0.0em">- Presented in AI for Content Creation Workshop (AI4CC) at CVPR 2024</small><br>
            <small style="color:gray;margin-bottom: 0.0em">- Presented in AI for 3D Generation Workshop (AI3DG) at CVPR 2024</small>
          </p>
        </div>

        <div class="item selected">
          <img src='./media/metta.gif'/>
          <p>
            <b class="title">MeTTA: Single-View to 3D Textured Mesh Reconstruction with Test-Time Adaptation</b><br/>
            Kim Yu-Ji, Hyunwoo Ha, <u>Kim Youwang</u>, Jaeheung Surh, Hyowon Ha, Tae-Hyun Oh<br>
            <i>BMVC 2024 (<span style="color:#c00000;">Best Poster Award</span>)</i><br/>
              <a href="https://metta3d.github.io/" target="_blank">Project page</a><br>
            <small style="color:gray;margin-bottom: 0.0em">- Best Poster Award at BMVC 2024</small><br>
          </p>
        </div>

        <div class="item">
          <img src='./media/fprf_v2.gif'/>
          <p>
            <b class="title">Feed-Forward Photorealistic Style Transfer for Large-Scale 3D Neural Radiance Fields</b><br/>
            GeonU Kim, <u>Kim Youwang</u>, Tae-Hyun Oh<br/>
            <i>AAAI 2024</i><br/>
              <a href="https://kim-geonu.github.io/FPRF/" target="_blank">Project page</a> |
              <a href="https://arxiv.org/abs/2401.05516" target="_blank">Paper</a> |
              <a href="https://github.com/kaist-ami/FPRF" target="_blank">Code</a><br>
          </p>
        </div>


        <div class="item">
          <img src='./media/rank_pruning.jpg'/>
          <p>
            <b class="title">Multi-stage Adaptive Rank Statistic Pruning for Lightweight Human 3D Mesh Recovery Model</b><br/>
            Dong Hun Ryou, <u>Kim Youwang</u>, Tae-Hyun Oh<br/>
            <i>The Visual Computer Journal (TVCJ) 2023</i><br/>
              <a href="https://link.springer.com/article/10.1007/s00371-023-02798-x" target="_blank">Paper</a>
          </p>
        </div>

        <div class="item selected">
          <img src='./media/clip_actor.gif'/>
          <p>
            <b class="title">CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes</b><br/>
            <u>Kim Youwang</u>*, Kim Ji-Yeon*, Tae-Hyun Oh<br/>
            <i>ECCV 2022</i><br/>
              <a href="https://clip-actor.github.io" target="_blank">Project page</a> |
              <a href="https://arxiv.org/abs/2206.04382" target="_blank">Paper</a> |
              <a href="https://youtu.be/oWr4NP-eVLY" target="_blank">Video</a> |
              <a href="https://github.com/kaist-ami/CLIP-Actor" target="_blank">Code</a> |
              <a href="https://www.dropbox.com/s/8l2jvvc0po6szn7/3229-poster.pdf?dl=0" target="_blank">Poster</a><br/>
              <small style="color:gray;margin-bottom:0.0em">- Grand Prize (1st place, Minister's award, $12K) at ICT paper awards 2023 </small><br/>
              <small style="color:gray;margin-bottom:0.0em">- Qualcomm Innovation Award Winner 2022</small><br/>
              <small style="color:gray;margin-bottom:0.0em">- Presented in AI for Content Creation Workshop (AI4CC) at CVPR 2023</small>
          </p>
        </div>

        <div class="item selected">
          <img src='./media/fastmetro_teaser.jpg'/>
          <p>
            <b class="title">FastMETRO: Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers</b><br/>
            Junhyeong Cho, <u>Kim Youwang</u>, Tae-Hyun Oh<br/>
            <i>ECCV 2022</i><br/>
              <a href="https://fastmetro.github.io/" target="_blank">Project page</a> |
              <a href="https://arxiv.org/abs/2207.13820" target="_blank">Paper</a> |
              <a href="https://github.com/kaist-ami/FastMETRO" target="_blank">Code</a> |
              <a href="https://www.dropbox.com/s/kzmihz488qcelxi/2116-poster.pdf?dl=0" target="_blank">Poster</a><br/>
          </p>
        </div>

        <div class="item">
          <img src='./media/demr.jpg'/>
          <p>
            <b class="title">Unified 3D Mesh Recovery of Humans and Animals by Learning Animal Exercise</b><br/>
            <u>Kim Youwang</u>, Kim Ji-Yeon, Kyungdon Joo, Tae-Hyun Oh<br/>
                <i>BMVC 2021</i><br/>
                <a href="https://demr-bmvc21.github.io" target="_blank">Project page</a> |
                <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0926.pdf" target="_blank">Paper</a>
          </p>
        </div>
      </div>
      <h2>Awards & Honors</h2>
        <ul>
            <li class="yw_list">Best Poster Award, BMVC, 2024</li>
            <li class="yw_list">Excellence Prize, Electronics Times ICT Paper Awards, 2024</li>
            <li class="yw_list">Best Poster Award, POSTECH-KAIST joint ML workshop, 2024</li>
            <li class="yw_list">Grand Prize (Minister's award, $12,000 prize), Electronics Times ICT Paper Awards, 2023</li>
            <li class="yw_list">Outstanding Reviewer Award, ICCV, 2023</li>
            <li class="yw_list">Winner ($4,000 prize), Qualcomm Innovation Fellowship Korea (QIFK), 2022</li>
            <li class="yw_list">International Computer Vision Summer School (ICVSS), 2022</li>
        </ul>

<!--      <h2>Talks</h2>-->
<!--        <ul>-->
<!--            <li class="yw_list">Towards Efficient & Realistic Virtual World Communication, INNERVERZ, Korea, 2023</li>-->
<!--        </ul>-->

      <h2>Academic Services</h2>
        <ul>
            <li class="yw_list">
              <b>Conference Reviewer</b>
              <ul>
                <li class="yw_list">CVPR: 2024, 2025, 2026</li>
                <li class="yw_list">ICCV: 2023 (<a href="https://iccv2023.thecvf.com/outstanding.reviewers-118.php">Outstanding reviewer</a>), 2025 </li>
                <li class="yw_list">ECCV: 2024</li>
                <li class="yw_list">NeurIPS: 2024, 2025</li>
                <li class="yw_list">BMVC: 2024</li>
                <br>
              </ul>
            </li>
            <li class="yw_list">
              <b>Journal Reviewer</b>
              <ul>
                <li class="yw_list">IEEE Trans. on Pattern Analysis and Machine Learning (TPAMI): 2024, 2025</li>
                <li class="yw_list">ACM Trans. on Graphics / SIGGRAPH Asia: 2024</li>
                <li class="yw_list">International Journal of Computer Vision (IJCV): 2024, 2025</li>
                <li class="yw_list">Transactions on Machine Learning Research (TMLR): 2025</li>
                <li class="yw_list">IEEE Transactions on Multimedia (TMM): 2023</li>
              </ul>
            </li>
        </ul>

    </div> <!-- container -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5RW688RKHW"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-5RW688RKHW');
    </script>
    <script>
      // Wait until the DOM is fully loaded to run the script
      document.addEventListener('DOMContentLoaded', function() {
        const allBtn = document.getElementById('all-btn');
        const selectedBtn = document.getElementById('selected-btn');
        // Get all publication items within the dedicated section
        const publications = document.querySelectorAll('.publications-section .item');

        function filterPublications(filter) {
          publications.forEach(pub => {
            // Logic to show or hide publications based on the filter
            if (filter === 'all') {
              // Show all publications
              pub.style.display = 'block';
            } else if (filter === 'selected') {
              // Show only publications with the 'selected' class
              if (pub.classList.contains('selected')) {
                pub.style.display = 'block';
              } else {
                pub.style.display = 'none';
              }
            }
          });

          // Update the active state of the buttons
          if (filter === 'all') {
            allBtn.classList.add('active-button');
            selectedBtn.classList.remove('active-button');
          } else {
            selectedBtn.classList.add('active-button');
            allBtn.classList.remove('active-button');
          }
        }

        // Add click event listeners to the buttons
        allBtn.addEventListener('click', () => filterPublications('all'));
        selectedBtn.addEventListener('click', () => filterPublications('selected'));

        // Initially, show selected publications when the page loads
        filterPublications('selected');
      });
    </script>
  </body>
</html>
